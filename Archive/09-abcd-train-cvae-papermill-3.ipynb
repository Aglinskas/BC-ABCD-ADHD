{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9541d6a2",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca02545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T06:11:29.446571Z",
     "iopub.status.busy": "2023-03-27T06:11:29.446199Z",
     "iopub.status.idle": "2023-03-27T06:11:29.453791Z",
     "shell.execute_reply": "2023-03-27T06:11:29.453410Z"
    },
    "papermill": {
     "duration": 0.024519,
     "end_time": "2023-03-27T06:11:29.453925",
     "exception": false,
     "start_time": "2023-03-27T06:11:29.429406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mmfs1/data/aglinska/BC-ABCD-ADHD'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5475fee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T06:11:29.470322Z",
     "iopub.status.busy": "2023-03-27T06:11:29.469976Z",
     "iopub.status.idle": "2023-03-27T06:11:29.471745Z",
     "shell.execute_reply": "2023-03-27T06:11:29.472025Z"
    },
    "papermill": {
     "duration": 0.010526,
     "end_time": "2023-03-27T06:11:29.472128",
     "exception": false,
     "start_time": "2023-03-27T06:11:29.461602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-27 02:11:29.469058\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime;now = datetime.now;t00 = now()\n",
    "print(t00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1137df",
   "metadata": {
    "papermill": {
     "duration": 0.006802,
     "end_time": "2023-03-27T06:11:29.484514",
     "exception": false,
     "start_time": "2023-03-27T06:11:29.477712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GPU CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77206b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T06:11:29.498621Z",
     "iopub.status.busy": "2023-03-27T06:11:29.498286Z",
     "iopub.status.idle": "2023-03-27T06:11:29.499780Z",
     "shell.execute_reply": "2023-03-27T06:11:29.500051Z"
    },
    "papermill": {
     "duration": 0.010194,
     "end_time": "2023-03-27T06:11:29.500146",
     "exception": false,
     "start_time": "2023-03-27T06:11:29.489952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac1aaaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T06:11:29.516639Z",
     "iopub.status.busy": "2023-03-27T06:11:29.516230Z",
     "iopub.status.idle": "2023-03-27T06:11:45.848656Z",
     "shell.execute_reply": "2023-03-27T06:11:45.847821Z"
    },
    "papermill": {
     "duration": 16.342269,
     "end_time": "2023-03-27T06:11:45.848758",
     "exception": false,
     "start_time": "2023-03-27T06:11:29.506489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 02:11:30.178496: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aglinska/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 02:11:39.252310: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-27 02:11:39.274059: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2400000000 Hz\n",
      "2023-03-27 02:11:39.277081: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555558715980 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-27 02:11:39.277115: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-03-27 02:11:39.285577: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2023-03-27 02:11:39.484923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555572b6a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-27 02:11:39.484957: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2023-03-27 02:11:39.485620: I tensorflow/core/common_runtime/gpu/gpu_device."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-27 02:11:39.485664: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-27 02:11:39.492635: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-27 02:11:39.495562: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-27 02:11:39.496744: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-27 02:11:39.499156: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-27 02:11:39.504709: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-27 02:11:39.508557: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-27 02:11:39.509626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-03-27 02:11:39.509670: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-27 02:11:40.488148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-27 02:11:40.488197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-03-27 02:11:40.488203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-03-27 02:11:40.489382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 2343 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:18:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "# GPU checks\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5edfc811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T06:11:45.870468Z",
     "iopub.status.busy": "2023-03-27T06:11:45.869898Z",
     "iopub.status.idle": "2023-03-27T06:11:46.240607Z",
     "shell.execute_reply": "2023-03-27T06:11:46.240918Z"
    },
    "papermill": {
     "duration": 0.381643,
     "end_time": "2023-03-27T06:11:46.241054",
     "exception": false,
     "start_time": "2023-03-27T06:11:45.859411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 27 02:11:45 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:18:00.0 Off |                    0 |\r\n",
      "| N/A   62C    P0   282W / 300W |  13519MiB / 16160MiB |    100%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   49C    P0   260W / 300W |  12858MiB / 16160MiB |    100%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\r\n",
      "| N/A   51C    P0   271W / 300W |  12858MiB / 16160MiB |    100%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:AF:00.0 Off |                    0 |\r\n",
      "| N/A   62C    P0   280W / 300W |  12866MiB / 16160MiB |    100%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A   3361670      C   ...nvs/syn_detect/bin/python    13063MiB |\r\n",
      "|    0   N/A  N/A   3398630      C   ...nska/anaconda3/bin/python      451MiB |\r\n",
      "|    1   N/A  N/A   3361671      C   ...nvs/syn_detect/bin/python    12855MiB |\r\n",
      "|    2   N/A  N/A   3361672      C   ...nvs/syn_detect/bin/python    12855MiB |\r\n",
      "|    3   N/A  N/A   3361673      C   ...nvs/syn_detect/bin/python    12863MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# GPU checks\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1662ff",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dde6e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T06:11:46.273640Z",
     "iopub.status.busy": "2023-03-27T06:11:46.273201Z",
     "iopub.status.idle": "2023-03-27T06:11:51.639655Z",
     "shell.execute_reply": "2023-03-27T06:11:51.638855Z"
    },
    "papermill": {
     "duration": 5.392222,
     "end_time": "2023-03-27T06:11:51.639809",
     "exception": true,
     "start_time": "2023-03-27T06:11:46.247587",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 02:11:46.261138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-27 02:11:46.261203: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-27 02:11:46.261231: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-27 02:11:46.261240: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-27 02:11:46.261250: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-27 02:11:46.261260: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-27 02:11:46.261268: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-27 02:11:46.261278: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-27 02:11:46.262089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-03-27 02:11:46.262130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-27 02:11:46.262136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-03-27 02:11:46.262140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-03-27 02:11:46.262962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 2343 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:18:00.0, compute capability: 7.0)\n",
      "2023-03-27 02:11:46.264306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-27 02:11:46.264333: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-27 02:11:46.264348: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-27 02:11:46.264356: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-27 02:11:46.264363: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-27 02:11:46.264371: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-27 02:11:46.264378: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-27 02:11:46.264386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-27 02:11:46.265110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-03-27 02:11:46.266132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-27 02:11:46.266154: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-27 02:11:46.266170: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-27 02:11:46.266180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-27 02:11:46.266190: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-27 02:11:46.266199: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-27 02:11:46.266210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-27 02:11:46.266219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-27 02:11:46.266983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-03-27 02:11:46.267003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-27 02:11:46.267008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-03-27 02:11:46.267011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-03-27 02:11:46.267796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2343 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:18:00.0, compute capability: 7.0)\n",
      "2023-03-27 02:11:46.409262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 02:11:47.582817: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-03-27 02:11:47.609228: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/aglinska/ipykernel_3398630/956670965.py\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Run the op several times.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/aglinska/ipykernel_3398630/956670965.py\u001b[0m in \u001b[0;36mgpu\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mrandom_image_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mnet_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_image_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     name=None):\n\u001b[0;32m-> 1010\u001b[0;31m   return convolution_internal(\n\u001b[0m\u001b[1;32m   1011\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m       return op(\n\u001b[0m\u001b[1;32m   1141\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2582\u001b[0m     \u001b[0;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m     \u001b[0;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m     return gen_nn_ops.conv2d(\n\u001b[0m\u001b[1;32m   2585\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2586\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    936\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "# Run GPU test\n",
    "import tensorflow as tf\n",
    "import timeit,pickle\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46788949",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Imports and Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc5a42",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from importlib import reload\n",
    "#from helper_funcs import *\n",
    "#from make_models2 import *\n",
    "\n",
    "# Make tqdm work for notebooks\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "print(now()-t00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c671b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "      args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "      z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def get_MRI_VAE_3D(input_shape=(64,64,64,1),\n",
    "                   latent_dim=2,\n",
    "                   batch_size = 32,\n",
    "                   disentangle=False,\n",
    "                   gamma=1,\n",
    "                   kernel_size = 3,\n",
    "                   filters = 16,\n",
    "                   intermediate_dim = 128,\n",
    "                   opt=None):\n",
    "    #TODO: add discriminator loss, see if there is improvement. Perhaps try on shapes dataset if it's easier...\n",
    "\n",
    "    image_size, _, _, channels = input_shape\n",
    "    \n",
    "    #epochs = 10\n",
    "    nlayers = 2\n",
    "      \n",
    "      # VAE model = encoder + decoder\n",
    "      # build encoder model\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = inputs\n",
    "    for i in range(nlayers):\n",
    "        filters *= 2\n",
    "        x = Conv3D(filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                activation='relu',\n",
    "                strides=2,\n",
    "                padding='same')(x)\n",
    "\n",
    "    # shape info needed to build decoder model\n",
    "    shape = K.int_shape(x)\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(intermediate_dim, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # instantiate encoder model\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    # build decoder model\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "    x = Dense(shape[1] * shape[2] * shape[3] * shape[4], activation='relu')(x)\n",
    "    x = Reshape((shape[1], shape[2], shape[3],shape[4]))(x)\n",
    "\n",
    "    for i in range(nlayers):\n",
    "        x = Conv3DTranspose(filters=filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          strides=2,\n",
    "                          padding='same')(x)\n",
    "        filters //= 2\n",
    "\n",
    "    outputs = Conv3DTranspose(filters=1,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='sigmoid',\n",
    "                            padding='same',\n",
    "                            name='decoder_output')(x)\n",
    "\n",
    "    # instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    #     decoder.summary()\n",
    "\n",
    "    # instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    if disentangle:\n",
    "        discriminator = Dense(1, activation='sigmoid')\n",
    "\n",
    "        z1 = Lambda(lambda x: x[:int(batch_size/2),:int(latent_dim/2)])(z)\n",
    "        z2 = Lambda(lambda x: x[int(batch_size/2):,:int(latent_dim/2)])(z)\n",
    "        s1 = Lambda(lambda x: x[:int(batch_size/2),int(latent_dim/2):])(z)\n",
    "        s2 = Lambda(lambda x: x[int(batch_size/2):,int(latent_dim/2):])(z)\n",
    "        \n",
    "        q_bar = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "          axis=0)\n",
    "        q = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "          axis=0)\n",
    "        \n",
    "#         q_bar_score = discriminator(q_bar)\n",
    "#         q_score = discriminator(q)        \n",
    "#         tc_loss = K.log(q_score / (1 - q_score)) \n",
    "\n",
    "        q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1\n",
    "        q_score = (discriminator(q)+.1) *.85 \n",
    "        tc_loss = K.log(q_score / (1 - q_score)) \n",
    "\n",
    "        discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "\n",
    "    reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "    reconstruction_loss *= image_size * image_size\n",
    "\n",
    "\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    if disentangle:\n",
    "        vae_loss = K.mean(reconstruction_loss) + K.mean(kl_loss) + gamma * K.mean(tc_loss) + K.mean(discriminator_loss)\n",
    "    else:\n",
    "        vae_loss = K.mean(reconstruction_loss) + K.mean(kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    \n",
    "    if type(opt)==type(None):\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')\n",
    "        \n",
    "    #vae.compile(optimizer='rmsprop')\n",
    "    vae.compile(optimizer=opt)\n",
    "    \n",
    "\n",
    "    if disentangle:\n",
    "        vae.metrics_tensors = [reconstruction_loss, kl_loss, tc_loss, discriminator_loss]\n",
    "        #     vae.summary()\n",
    "    return encoder, decoder, vae\n",
    "\n",
    "\n",
    "\n",
    "def get_MRI_CVAE_3D(input_shape=(64,64,64,1),\n",
    "                    latent_dim=2,\n",
    "                    beta=1,\n",
    "                    disentangle=False,\n",
    "                    gamma=1,\n",
    "                    bias=True,\n",
    "                    batch_size = 64,\n",
    "                    kernel_size = 3,\n",
    "                    filters = 32,\n",
    "                    intermediate_dim = 128,\n",
    "                    opt=None):\n",
    "\n",
    "    image_size, _, _, channels = input_shape\n",
    "    #epochs = 10\n",
    "    nlayers = 2\n",
    "\n",
    "    # build encoder model\n",
    "    tg_inputs = Input(shape=input_shape, name='tg_inputs')\n",
    "    bg_inputs = Input(shape=input_shape, name='bg_inputs')\n",
    "\n",
    "    z_conv1 = Conv3D(filters=filters*2,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides=2,\n",
    "            use_bias=bias,\n",
    "            padding='same')\n",
    "\n",
    "    z_conv2 = Conv3D(filters=filters*4,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides=2,\n",
    "            use_bias=bias,\n",
    "            padding='same')\n",
    "\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    z_h_layer = Dense(intermediate_dim, activation='relu', use_bias=bias)\n",
    "    z_mean_layer = Dense(latent_dim, name='z_mean', use_bias=bias)\n",
    "    z_log_var_layer = Dense(latent_dim, name='z_log_var', use_bias=bias)\n",
    "    z_layer = Lambda(sampling, output_shape=(latent_dim,), name='z')\n",
    "\n",
    "    def z_encoder_func(inputs):\n",
    "        z_h = inputs\n",
    "        z_h = z_conv1(z_h)\n",
    "        z_h = z_conv2(z_h)\n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(z_h)\n",
    "        z_h = Flatten()(z_h)\n",
    "        z_h = z_h_layer(z_h)\n",
    "        z_mean =  z_mean_layer(z_h)\n",
    "        z_log_var =  z_log_var_layer(z_h)\n",
    "        z = z_layer([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z, shape\n",
    "\n",
    "    tg_z_mean, tg_z_log_var, tg_z, shape_z = z_encoder_func(tg_inputs)\n",
    "\n",
    "\n",
    "    s_conv1 = Conv3D(filters=filters*2,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides=2,\n",
    "            use_bias=bias,\n",
    "            padding='same')\n",
    "\n",
    "    s_conv2 = Conv3D(filters=filters*4,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides=2,\n",
    "            use_bias=bias,\n",
    "            padding='same')\n",
    "\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    s_h_layer = Dense(intermediate_dim, activation='relu', use_bias=bias)\n",
    "    s_mean_layer = Dense(latent_dim, name='s_mean', use_bias=bias)\n",
    "    s_log_var_layer = Dense(latent_dim, name='s_log_var', use_bias=bias)\n",
    "    s_layer = Lambda(sampling, output_shape=(latent_dim,), name='s')\n",
    "\n",
    "    def s_encoder_func(inputs):\n",
    "        s_h = inputs\n",
    "        s_h = s_conv1(s_h)\n",
    "        s_h = s_conv2(s_h)\n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(s_h)\n",
    "        s_h = Flatten()(s_h)\n",
    "        s_h = s_h_layer(s_h)\n",
    "        s_mean =  s_mean_layer(s_h)\n",
    "        s_log_var =  s_log_var_layer(s_h)\n",
    "        s = s_layer([s_mean, s_log_var])\n",
    "        return s_mean, s_log_var, s, shape\n",
    "\n",
    "    tg_s_mean, tg_s_log_var, tg_s, shape_s = s_encoder_func(tg_inputs)\n",
    "    #bg_s_mean, bg_s_log_var, bg_s, _ = s_encoder_func(bg_inputs) # this is what they had \n",
    "    bg_z_mean, bg_z_log_var, bg_z, _ = z_encoder_func(bg_inputs) # Aidas and Stefano team hax\n",
    "\n",
    "\n",
    "      # instantiate encoder models\n",
    "    z_encoder = tf.keras.models.Model(tg_inputs, [tg_z_mean, tg_z_log_var, tg_z], name='z_encoder')\n",
    "    s_encoder = tf.keras.models.Model(tg_inputs, [tg_s_mean, tg_s_log_var, tg_s], name='s_encoder')\n",
    "\n",
    "\n",
    "      # build decoder model\n",
    "    latent_inputs = Input(shape=(2*latent_dim,), name='z_sampling')\n",
    "\n",
    "    x = Dense(intermediate_dim, activation='relu', use_bias=bias)(latent_inputs)\n",
    "    x = Dense(shape_z[1] * shape_z[2] * shape_z[3] * shape_z[4], activation='relu', use_bias=bias)(x)\n",
    "    x = Reshape((shape_z[1], shape_z[2], shape_z[3],shape_z[4]))(x)\n",
    "\n",
    "    for i in range(nlayers):\n",
    "        x = Conv3DTranspose(filters=filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          strides=2,\n",
    "                          use_bias=bias,\n",
    "                          padding='same')(x)\n",
    "        filters //= 2\n",
    "\n",
    "    outputs = Conv3DTranspose(filters=1,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='sigmoid',\n",
    "                            padding='same',\n",
    "                            use_bias=bias,\n",
    "                            name='decoder_output')(x)\n",
    "\n",
    "    # instantiate decoder model\n",
    "    cvae_decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "      # decoder.summary()\n",
    "\n",
    "    def zeros_like(x):\n",
    "        return tf.zeros_like(x)\n",
    "\n",
    "    tg_outputs = cvae_decoder(tf.keras.layers.concatenate([tg_z, tg_s], -1))\n",
    "    zeros = tf.keras.layers.Lambda(zeros_like)(tg_z)\n",
    "\n",
    "    bg_outputs = cvae_decoder(tf.keras.layers.concatenate([bg_z, zeros], -1)) # Aidas look into this, is this correct\n",
    "\n",
    " #   fg_outputs = cvae_decoder(tf.keras.layers.concatenate([tg_z, zeros], -1))\n",
    "\n",
    "    # instantiate VAE model\n",
    "    cvae = tf.keras.models.Model(inputs=[tg_inputs, bg_inputs], \n",
    "                              outputs=[tg_outputs, bg_outputs], \n",
    "                              name='contrastive_vae')\n",
    "\n",
    "#     cvae_fg = tf.keras.models.Model(inputs=tg_inputs, \n",
    "#                                   outputs=fg_outputs, \n",
    "#                                   name='contrastive_vae_fg')\n",
    "\n",
    "    if disentangle:\n",
    "        discriminator = Dense(1, activation='sigmoid')\n",
    "\n",
    "        z1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_z)\n",
    "        z2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_z)\n",
    "        s1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_s)\n",
    "        s2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_s)\n",
    "\n",
    "        q_bar = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1\n",
    "        q_score = (discriminator(q)+.1) *.85 \n",
    "        tc_loss = K.log(q_score / (1 - q_score)) \n",
    "        discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "    else:\n",
    "        tc_loss = 0\n",
    "        discriminator_loss = 0\n",
    "\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(tg_inputs), K.flatten(tg_outputs)) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(bg_inputs), K.flatten(bg_outputs)) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2] * input_shape[3]\n",
    "\n",
    "\n",
    "    kl_loss = 1 + tg_z_log_var - tf.keras.backend.square(tg_z_mean) - tf.keras.backend.exp(tg_z_log_var)\n",
    "    kl_loss += 1 + tg_s_log_var - tf.keras.backend.square(tg_s_mean) - tf.keras.backend.exp(tg_s_log_var)\n",
    "    kl_loss += 1 + bg_z_log_var - tf.keras.backend.square(bg_z_mean) - tf.keras.backend.exp(bg_z_log_var)\n",
    "    kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    \n",
    "    #print(f'reconstruction loss {reconstruction_loss}')\n",
    "    #print(f'kl_loss loss {kl_loss}')\n",
    "    #print(f'tc_loss loss {tc_loss}')\n",
    "    #print(f'discriminator_loss loss {discriminator_loss}')\n",
    "    \n",
    "    cvae_loss = tf.keras.backend.mean(reconstruction_loss + beta*kl_loss + gamma*tc_loss + discriminator_loss)\n",
    "    cvae.add_loss(cvae_loss)\n",
    "    \n",
    "    if type(opt)==type(None):\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')\n",
    "    \n",
    "#     opt = tf.keras.optimizers.SGD(\n",
    "#     learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD')\n",
    "\n",
    "    #opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.9, epsilon=1e-07, centered=False, name='RMSprop')\n",
    "    \n",
    "    #cvae.compile(optimizer='rmsprop',run_eagerly=True)\n",
    "    cvae.compile(optimizer=opt,run_eagerly=True)\n",
    "    \n",
    "\n",
    "    #return cvae, cvae_fg, z_encoder, s_encoder, cvae_decoder\n",
    "    return cvae, z_encoder, s_encoder, cvae_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2872f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_rsa(rdm_data,rdm_model):\n",
    "    return np.corrcoef(get_triu(rdm_data),get_triu(rdm_model))[0,1]\n",
    "        \n",
    "def make_RDM(inVec,data_scale='ratio',metric='euclidean'):\n",
    "    vec = inVec\n",
    "    vec = (vec - min(vec.flatten())) / (max(vec.flatten())-min(vec.flatten()))\n",
    "    \n",
    "    if np.ndim(inVec)==1: # must be at least 2D\n",
    "        vec = np.vstack((vec,np.zeros(vec.shape))).transpose()\n",
    "                   \n",
    "    mat = squareform(pdist(vec,metric=metric).transpose())\n",
    "\n",
    "    if data_scale=='ordinal':\n",
    "        mat[mat!=0]=1 # Make into zeros and ones\n",
    "        \n",
    "    return mat\n",
    "\n",
    "def get_triu(inMat):\n",
    "\n",
    "    assert np.ndim(inMat)==2, 'not 2 dim, wtf'\n",
    "    assert inMat.shape[0]==inMat.shape[1], 'not a square'\n",
    "\n",
    "    n = inMat.shape[0]\n",
    "    triu_vec = inMat[np.triu_indices(n=n,k=1)]\n",
    "    \n",
    "    return triu_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3727e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24097197",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nval = batch_size*3\n",
    "print(nval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0ecfd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load all data\n",
    "arr_adhd = np.load('./Data/brain_arr_64_ADHD_959.npz')\n",
    "arr_td = np.load('./Data/brain_arr_64_TD_3874.npz')\n",
    "\n",
    "print(list(arr_adhd.keys()))\n",
    "\n",
    "## Validation set\n",
    "data_val = np.concatenate((arr_adhd['arr'][0:nval,:,:,:],arr_td['arr'][0:nval,:,:,:]))\n",
    "subs_val = np.hstack((arr_adhd['subs'][0:nval],arr_td['subs'][0:nval]))\n",
    "patiens_val = np.array([sub in arr_adhd['subs'] for sub in subs_val])\n",
    "\n",
    "print('___Validation____')\n",
    "print(data_val.shape)\n",
    "print(subs_val.shape)\n",
    "print(patiens_val.sum())\n",
    "\n",
    "## Training set\n",
    "data = np.concatenate((arr_adhd['arr'][nval::,:,:,:],arr_td['arr'][nval::,:,:,:]))\n",
    "subs = np.hstack((arr_adhd['subs'][nval::],arr_td['subs'][nval::]))\n",
    "patients = np.array([sub in arr_adhd['subs'] for sub in subs])\n",
    "\n",
    "print('___Data____')\n",
    "print(data.shape)\n",
    "print(subs.shape)\n",
    "print(patients.sum())\n",
    "\n",
    "del arr_adhd\n",
    "del arr_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02a002",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rsa = pd.read_csv('df_rsa.csv')\n",
    "df_rsa['subID'] = ['sub-'+val.replace('_','') for val in df_rsa['subjectkey'].values] # add mathcing IDs\n",
    "df_rsa = df_rsa.iloc[[np.argwhere(subs[patients][i]==df_rsa['subID'].values)[0][0] for i in range(patients.sum())]] # slice ADHD and sort\n",
    "df_rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689ccdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdm_age_model = make_RDM(df_rsa['interview_age'].values,data_scale='ratio',metric='euclidean')\n",
    "rdm_sex_model = make_RDM(df_rsa['sexID'].values,data_scale='ordinal')\n",
    "rdm_scanner_model = make_RDM(df_rsa['scannerID'].values,data_scale='ordinal')\n",
    "rdm_symp_model = make_RDM(df_rsa['sumSympt'].values,data_scale='ratio',metric='euclidean')\n",
    "e = ~np.isnan(df_rsa['IQ'].values)\n",
    "rdm_IQ_model = make_RDM(df_rsa['IQ'].values[e],data_scale='ratio',metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db59292",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,5,1);plt.imshow(rdm_age_model);plt.title('rdm_age_model');\n",
    "plt.subplot(1,5,2);plt.imshow(rdm_sex_model);plt.title('rdm_sex_model');\n",
    "plt.subplot(1,5,3);plt.imshow(rdm_scanner_model);plt.title('rdm_scanner_model');\n",
    "plt.subplot(1,5,4);plt.imshow(rdm_symp_model);plt.title('rdm_symp_model');\n",
    "plt.subplot(1,5,5);plt.imshow(rdm_IQ_model);plt.title('rdm_IQ_model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e89c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005dc4c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac0274",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "class cvae_data_loader_adhd():\n",
    "    ''' this is the info'''\n",
    "    def __init__(self,data,patients,batch_size=32):\n",
    "    \n",
    "        self.data = data\n",
    "        self.patients = patients\n",
    "        self.n = data.shape[0]\n",
    "        self.epoch = -1\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.new_epoch()\n",
    "        self.n_batches = int(np.floor(min((len(self.adhd_idxs),len(self.td_idxs)))/self.batch_size)) # How many batches fit, take the min(n_ADHD,n_TD) then divide by batch size\n",
    "        \n",
    "    def new_epoch(self):\n",
    "\n",
    "        self.adhd_idxs = np.nonzero(self.patients==True)[0] # idxs of patients\n",
    "        self.td_idxs = np.nonzero(self.patients==False)[0] # idxs of TDs\n",
    "        \n",
    "        self.adhd_idxs = np.random.permutation(self.adhd_idxs)\n",
    "        self.td_idxs = np.random.permutation(self.td_idxs)\n",
    "        \n",
    "        self.epoch += 1\n",
    "        self.b = 0\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        self.b += 1\n",
    "        \n",
    "        if self.b==self.n_batches:\n",
    "            self.new_epoch()\n",
    "        \n",
    "        self.batch_adhd_idx = self.adhd_idxs[np.arange(self.b*self.batch_size,self.b*self.batch_size+self.batch_size)]\n",
    "        self.batch_td_idx = self.td_idxs[np.arange(self.b*self.batch_size,self.b*self.batch_size+self.batch_size)]\n",
    "        # go through the patients and controls in batch size chunks\n",
    "        # batch_indeces = all_indices[batch number * batch size : batch number * batch size + batch size]\n",
    "        \n",
    "        self.batch_adhd = self.data[self.batch_adhd_idx,:,:,:]\n",
    "        self.batch_td = self.data[self.batch_td_idx,:,:,:]\n",
    "        \n",
    "        _,counts = np.unique(np.hstack((self.batch_adhd_idx,self.batch_td_idx)),return_counts=True)\n",
    "        assert all(counts==1),'not all unique, somethings wrong'\n",
    "        \n",
    "        return self.batch_adhd,self.batch_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7130e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_triu(inMat):\n",
    "\n",
    "    assert np.ndim(inMat)==2, 'not 2 dim, wtf'\n",
    "    assert inMat.shape[0]==inMat.shape[1], 'not a square'\n",
    "\n",
    "    n = inMat.shape[0]\n",
    "    triu_vec = inMat[np.triu_indices(n=n,k=1)]\n",
    "    \n",
    "    return triu_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d0760",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cvae_dashboard():\n",
    "    \n",
    "    from IPython import display\n",
    "    import sys\n",
    "    \n",
    "    ### PROGRESS PLOTTING\n",
    "    display.clear_output(wait=True);\n",
    "    display.display(plt.gcf());\n",
    "    #Organise figure\n",
    "    ncols = 4;nrows=7\n",
    "    if np.mod(epoch,5)==0:\n",
    "        plt.close()\n",
    "    plt.subplots(nrows,ncols,figsize=(15,15)); # MAKE THE FIGURE\n",
    "\n",
    "    ### Generate Necessary files\n",
    "    patient_batch = adhd_batch\n",
    "    control_batch = td_batch\n",
    "    predictions = cvae.predict([adhd_batch, td_batch])\n",
    "    sigma = (np.e ** z_encoder.predict(adhd_batch)[1]).mean()\n",
    "    sigmas.append(sigma)\n",
    "\n",
    "    mu = z_encoder.predict(patient_batch)[0]\n",
    "    mus.append(np.mean([mu[:,0].std() for i in range(mu.shape[1])]))\n",
    "\n",
    "    prediction = predictions[0]\n",
    "\n",
    "    cmat_actual = np.corrcoef(np.vstack((patient_batch.reshape(patient_batch.shape[0],-1),control_batch.reshape(control_batch.shape[0],-1))))\n",
    "    cmat_pred = np.corrcoef(np.vstack((predictions[0].reshape(predictions[0].shape[0],-1),predictions[1].reshape(predictions[1].shape[0],-1))))\n",
    "    c_sim.append(np.corrcoef(get_triu(cmat_pred),get_triu(cmat_actual))[0,1])\n",
    "\n",
    "    \n",
    "    ### RSA Values\n",
    "    z = z_encoder.predict(data[patients,:,:,:])[2]\n",
    "    s = s_encoder.predict(data[patients,:,:,:])[2]\n",
    "\n",
    "    rdm_z = make_RDM(z,data_scale='ratio', metric='euclidean')\n",
    "    rdm_s = make_RDM(s,data_scale='ratio', metric='euclidean')\n",
    "\n",
    "    rsa_z = fit_rsa(rdm_z,rdm_scanner_model),fit_rsa(rdm_z,rdm_age_model),fit_rsa(rdm_z,rdm_sex_model),fit_rsa(rdm_z,rdm_symp_model)\n",
    "    rsa_s = [fit_rsa(rdm_s,rdm_scanner_model),fit_rsa(rdm_s,rdm_age_model),fit_rsa(rdm_s,rdm_sex_model),fit_rsa(rdm_s,rdm_symp_model)]\n",
    "\n",
    "    col_rsa_z.append(rsa_z)\n",
    "    col_rsa_s.append(rsa_s)\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 1 & 2 ##### \n",
    "\n",
    "    plt.subplot(nrows,ncols/2,1) # PLOT LOSS\n",
    "\n",
    "    plot_loss = loss[int(len(loss)*.2)::]\n",
    "    plot_loss_val = val_loss[int(len(loss)*.2)::]\n",
    "\n",
    "    xs = np.arange(len(plot_loss))+1\n",
    "    m,b = np.polyfit(xs,plot_loss,deg=1)\n",
    "    m_val,b_val = np.polyfit(xs,plot_loss_val,deg=1)\n",
    "\n",
    "    plt.plot(plot_loss)\n",
    "    plt.plot(plot_loss_val)\n",
    "    plt.plot(xs, m*xs + b)\n",
    "    plt.title(f'Epoch {data_loader.epoch} batch {data_loader.b}/{data_loader.n_batches} | Loss {loss[-1]:.2f},| vLoss {val_loss[-1]:.2f}, beta: {m:.4f}')\n",
    "\n",
    "    ##### SUBPLOT 3 ##### \n",
    "    plt.subplot(nrows,ncols,3) # PLOT LOSS LAST 50\n",
    "    hb = 500\n",
    "    if len(loss)>hb:\n",
    "        plot_loss = loss[-hb::]\n",
    "        plot_loss_val = val_loss[-hb::]\n",
    "\n",
    "        xs = np.arange(len(plot_loss))\n",
    "        m,b = np.polyfit(xs,plot_loss,deg=1)\n",
    "        m_val,b_val = np.polyfit(xs,plot_loss_val,deg=1)\n",
    "        plt.plot(plot_loss)\n",
    "        #plt.plot(plot_loss_val)\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        #plt.title(hist)\n",
    "        plt.title(f'Loss last {hb} it, beta {m:.4f}, vbeta {m_val:.4f}')\n",
    "\n",
    "    ##### SUBPLOT 4 ##### \n",
    "    plt.subplot(nrows,ncols,4)\n",
    "    plt.hist(prediction[0,:,:,0].flatten(),alpha=.5)\n",
    "    plt.hist(patient_batch[0,:,:].flatten(),alpha=.5)\n",
    "    plt.legend(['predicted','actual'])\n",
    "    plt.title('in/out histograms')\n",
    "\n",
    "    ##### SUBPLOT 5 ##### \n",
    "    plt.subplot(nrows,ncols,5) #RSA over time\n",
    "    plt.plot(c_sim)\n",
    "    plt.title(f'in/out RSA: {c_sim[-1].round(2)}')\n",
    "\n",
    "    if len(c_sim)>5: # PLOT LS LINE\n",
    "        xs = np.arange(len(c_sim))+1\n",
    "        m,b = np.polyfit(xs,c_sim,deg=1)\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        plt.title(f'in/out RSA: {c_sim[-1].round(2)}, b={m:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 6 ##### \n",
    "    plt.subplot(nrows,ncols,6)\n",
    "    if len(c_sim)>hb:\n",
    "        #plot_loss = loss[-hb::]\n",
    "        xs = np.arange(len(c_sim[-hb::]))\n",
    "        m,b = np.polyfit(xs,c_sim[-hb::],deg=1)\n",
    "        plt.plot(c_sim[-hb::])\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        #plt.title(hist)\n",
    "        plt.title(f'in/outRSA last {hb} it, b={m:.4f}')\n",
    "\n",
    "    # ##### SUBPLOT 7 ##### \n",
    "    plt.subplot(nrows,ncols,7)\n",
    "    plt.plot(np.array(col_rsa_z)[:,0])\n",
    "    plt.plot(np.array(col_rsa_s)[:,0])\n",
    "    plt.legend(['Z','S'])\n",
    "    plt.title('RSA scanner')\n",
    "\n",
    "    # ##### SUBPLOT 8 ##### \n",
    "    plt.subplot(nrows,ncols,8)\n",
    "    plt.plot(np.array(col_rsa_z)[:,3])\n",
    "    plt.plot(np.array(col_rsa_s)[:,3])\n",
    "    plt.legend(['Z','S'])\n",
    "    plt.title('RSA Symptom')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 9 ##### \n",
    "    plt.subplot(nrows,ncols,9)\n",
    "    plt.plot(sigmas)\n",
    "    plt.title(f'sigmas | {sigmas[-1]:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 10 ##### \n",
    "    plt.subplot(nrows,ncols,10)\n",
    "    plt.plot(mus)\n",
    "    plt.title(f'Mu variance {mus[-1]:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 11 ##### \n",
    "    plt.subplot(nrows,ncols,11)\n",
    "    sns.heatmap(cmat_actual,xticklabels=[],yticklabels=[])\n",
    "    plt.title('input RSA')\n",
    "\n",
    "    ##### SUBPLOT 12 ##### \n",
    "    plt.subplot(nrows,ncols,12)\n",
    "    sns.heatmap(cmat_pred,xticklabels=[],yticklabels=[])\n",
    "    plt.title('output RSA')\n",
    "\n",
    "    # #############################################\n",
    "    # ###################Reconstructions###########\n",
    "    # #############################################\n",
    "\n",
    "    ##### SUBPLOT 13 #####     \n",
    "    rand_sub = np.random.randint(low=0,high=patient_batch.shape[0])\n",
    "    rand_sub = 0\n",
    "\n",
    "    #### AXIAL SLICES  ####\n",
    "    plt.subplot(nrows,ncols,13)\n",
    "    #sns.heatmap(patient_batch[rand_sub,:,:])\n",
    "    plt.imshow(patient_batch[rand_sub,:,:,32])\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('actual')\n",
    "    ##### SUBPLOT 14 #####     \n",
    "    plt.subplot(nrows,ncols,14)\n",
    "    #sns.heatmap(prediction[rand_sub,:,:,0])\n",
    "    plt.imshow(prediction[rand_sub,:,:,32,0])\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "    # ##### SUBPLOT 15 #####     \n",
    "    plt.subplot(nrows,ncols,15)\n",
    "    plt.imshow(abs(patient_batch[rand_sub,:,:,32]-prediction[rand_sub,:,:,32,0]))\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('difference')\n",
    "    \n",
    "    \n",
    "    plt.subplot(nrows,ncols,16)\n",
    "    plt.plot(np.array(col_rsa_z)[-1,:],'b.',markersize=20);\n",
    "    plt.plot(np.array(col_rsa_s)[-1,:],'g.',markersize=20);\n",
    "    plt.xticks(np.arange(4),labels=['scanner','age','sex','sympt']);\n",
    "    plt.title('RSA')\n",
    "\n",
    "    #### SAGITAL SLICES  ####\n",
    "    plt.subplot(nrows,ncols,17)\n",
    "    #sns.heatmap(patient_batch[rand_sub,:,:])\n",
    "    plt.imshow(np.rot90(patient_batch[rand_sub,32,:,:]))\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('actual')\n",
    "    ##### SUBPLOT 14 #####     \n",
    "    plt.subplot(nrows,ncols,18)\n",
    "    #sns.heatmap(prediction[rand_sub,:,:,0])\n",
    "    plt.imshow(np.rot90(prediction[rand_sub,32,:,:,0]))\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "    # ##### SUBPLOT 15 #####     \n",
    "    plt.subplot(nrows,ncols,19)\n",
    "    plt.imshow(np.rot90( abs(patient_batch[rand_sub,32,:,:]-prediction[rand_sub,32,:,:,0]) ))\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('difference')\n",
    "\n",
    "\n",
    "\n",
    "    # ##### SUBPLOT 16 #####                                             \n",
    "    # plt.subplot(nrows,ncols,16)\n",
    "    # plt.imshow(np.rot90(prediction[rand_sub,16,:,:,rand_map]))\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "    # ##### SUBPLOT 17 #####     \n",
    "    #     plt.subplot(nrows,ncols,18)\n",
    "    #     keys = ['dataset_id','siteID','age','gender','fiq','ados_total','ados_social','ados_comm','ados_rrb',]\n",
    "    #     scales = ['ordinal','ordinal','ratio','ordinal','ratio','ratio','ratio','ratio','ratio',]\n",
    "    #     rsa_res = np.array([key_rsa_cvae(keys[i],scales[i],cmats_val,df_val) for i in range(len(keys))])\n",
    "    #     plt.plot(rsa_res[:,0],'.',markersize=15,alpha=.5)\n",
    "    #     plt.plot(rsa_res[:,1],'.',markersize=15,alpha=.5)\n",
    "    #     plt.legend(['Z','S']);\n",
    "    #     plt.xticks(np.arange(rsa_res.shape[0]),labels=keys,rotation=45);\n",
    "    #     plt.title('RSA VAL')\n",
    "\n",
    "    # ##### SUBPLOT 18 #####     \n",
    "    #plt.subplot(nrows,ncols,19)\n",
    "    #keys = ['dataset_id','siteID','age','gender','fiq','ados_total','ados_social','ados_comm','ados_rrb',]\n",
    "    #scales = ['ordinal','ordinal','ratio','ordinal','ratio','ratio','ratio','ratio','ratio',]\n",
    "    #rsa_res = np.array([key_rsa_cvae(keys[i],scales[i],cmats_test,df_test) for i in range(len(keys))])\n",
    "    #plt.plot(rsa_res[:,0],'.',markersize=15,alpha=.5)\n",
    "    #plt.plot(rsa_res[:,1],'.',markersize=15,alpha=.5)\n",
    "    #plt.legend(['Z','S']);\n",
    "    #plt.xticks(np.arange(rsa_res.shape[0]),labels=keys,rotation=45);\n",
    "    #plt.title('RSA TEST')\n",
    "\n",
    "    # ##### SUBPLOT 19 #####     \n",
    "    # plt.subplot(nrows,ncols,19)\n",
    "    # plt.imshow(patient_batch[rand_sub,:,:,16,rand_map])\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('actual')\n",
    "    # ##### SUBPLOT 20 #####     \n",
    "    # plt.subplot(nrows,ncols,20)\n",
    "    # plt.imshow(prediction[rand_sub,:,:,16,rand_map])\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "\n",
    "\n",
    "    # #############################################\n",
    "    # ################### LOSSES ##################\n",
    "    # #############################################\n",
    "\n",
    "\n",
    "    # Validation loss\n",
    "    #predictions = cvae.predict([patient_batch_val,control_batch_val])\n",
    "    #input_shape = data_size[1:]\n",
    "    #reconstruction_loss = tf.keras.losses.mse(K.flatten(patient_batch_val), K.flatten(predictions[0])) \n",
    "    #reconstruction_loss += tf.keras.losses.mse(K.flatten(control_batch_val), K.flatten(predictions[1])) \n",
    "    #reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "    #val_mse.append(reconstruction_loss)\n",
    "\n",
    "\n",
    "    predictions = cvae.predict([patient_batch,control_batch])\n",
    "    #input_shape = data_size[1:]\n",
    "    input_shape = data.shape[1::]\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(patient_batch), K.flatten(predictions[0])) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(control_batch), K.flatten(predictions[1])) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "\n",
    "\n",
    "    tg_z_mean, tg_z_log_var, tg_z = z_encoder.predict(patient_batch)\n",
    "    tg_s_mean, tg_s_log_var, tg_s = s_encoder.predict(patient_batch)\n",
    "\n",
    "    bg_z_mean, bg_z_log_var, bg_z = z_encoder.predict(control_batch)\n",
    "\n",
    "    kl_loss1 = 1 + tg_z_log_var - tf.keras.backend.square(tg_z_mean) - tf.keras.backend.exp(tg_z_log_var)\n",
    "    kl_loss2 = 1 + tg_s_log_var - tf.keras.backend.square(tg_s_mean) - tf.keras.backend.exp(tg_s_log_var)\n",
    "    kl_loss3 = 1 + bg_z_log_var - tf.keras.backend.square(bg_z_mean) - tf.keras.backend.exp(bg_z_log_var)\n",
    "\n",
    "    kl_loss1 = tf.keras.backend.sum(kl_loss1, axis=-1)\n",
    "    kl_loss2 = tf.keras.backend.sum(kl_loss2, axis=-1)\n",
    "    kl_loss3 = tf.keras.backend.sum(kl_loss3, axis=-1)\n",
    "    kl_loss = kl_loss1+kl_loss2+kl_loss3\n",
    "    kl_loss *= -0.5\n",
    "\n",
    "\n",
    "    discriminator = Dense(1, activation='sigmoid')\n",
    "    z1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_z)\n",
    "    z2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_z)\n",
    "    s1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_s)\n",
    "    s2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_s)\n",
    "\n",
    "    q_bar = tf.keras.layers.concatenate(\n",
    "      [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "      tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "      axis=0)\n",
    "\n",
    "    q = tf.keras.layers.concatenate(\n",
    "      [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "      tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "      axis=0)\n",
    "\n",
    "    q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1\n",
    "    q_score = (discriminator(q)+.1) *.85 \n",
    "    tc_loss = K.log(q_score / (1 - q_score)) \n",
    "    discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "    discriminator_loss\n",
    "\n",
    "    loss_mse.append(reconstruction_loss.numpy())\n",
    "    loss_kl.append(kl_loss.numpy().mean())\n",
    "    loss_dc.append(tc_loss.numpy().mean())\n",
    "    loss_tc.append(discriminator_loss.numpy().mean())\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,21) # MSE \n",
    "    plt.plot(loss_mse[int(len(loss_mse)*.2)::])\n",
    "    #plt.plot(val_mse[int(len(loss_mse)*.2)::])\n",
    "\n",
    "    plt.title(f'MSE | {loss_mse[-1]:.4f}')\n",
    "\n",
    "    plt.subplot(nrows,ncols,22) # KL loss\n",
    "    plt.plot(loss_kl)\n",
    "    plt.title(f'KL | {loss_kl[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,23) # TC     \n",
    "    plt.plot(loss_tc)\n",
    "    plt.title(f'Total Correlation loss | {loss_tc[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,24) # Disc         \n",
    "    plt.plot(loss_dc)\n",
    "    plt.title(f'discriminator_loss | {loss_dc[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    tg_s = s_encoder.predict(patient_batch)\n",
    "    tg_z = z_encoder.predict(patient_batch)\n",
    "    bg_z = z_encoder.predict(control_batch)\n",
    "\n",
    "    plt.subplot(nrows,ncols,25)\n",
    "    plt.hist(tg_s[2].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[2].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[2].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Z')\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,26)\n",
    "    plt.hist(tg_s[0].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[0].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[0].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Mus')\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,27)\n",
    "    plt.hist(tg_s[1].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[1].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[1].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Sigmas')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7bc0a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549cdf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eebd5d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Define the model\n",
    "latent_dim = 16\n",
    "beta = 1;gamma = 100\n",
    "disentangle = True\n",
    "cvae, z_encoder, s_encoder, cvae_decoder = get_MRI_CVAE_3D(input_shape=(64,64,64,1),latent_dim=latent_dim,beta=beta, disentangle=disentangle, gamma=gamma, bias=True, batch_size = batch_size)\n",
    "\n",
    "fdir = './Data/tf_outputs/CVAE/'\n",
    "fn = 'CVAE_weights_test_3'\n",
    "\n",
    "fn = os.path.join(fdir,fn)\n",
    "loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df75568",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize loaders\n",
    "data_loader = cvae_data_loader_adhd(data,patients)\n",
    "data_loader_val = cvae_data_loader_adhd(data_val,patiens_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba0780",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Initalize tracking variables\n",
    "sigmas = []\n",
    "mus = []\n",
    "c_sim = []\n",
    "\n",
    "loss_mse = []\n",
    "loss_kl = []\n",
    "loss_dc = []\n",
    "loss_tc = []\n",
    "\n",
    "col_rsa_z = []\n",
    "col_rsa_s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1821e01",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "n_batches = data_loader.n_batches # dataloader calcs how many batches\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for batch in range(n_batches):\n",
    "        \n",
    "        adhd_batch, td_batch = data_loader.get_batch() # Get batches\n",
    "        l = cvae.train_on_batch([adhd_batch,td_batch]) # [TG,BG]\n",
    "        loss.append(l)\n",
    "        \n",
    "        # Validation loss\n",
    "        vl = np.array([cvae.test_on_batch(data_loader_val.get_batch()) for _ in range(data_loader_val.n_batches)]).mean()\n",
    "        val_loss.append(vl) # put it in a list \n",
    "        \n",
    "        if batch == 0: # Checkpointing\n",
    "            cvae_dashboard()\n",
    "            cvae.save_weights(fn) \n",
    "            np.save(os.path.join(fdir,'loss_10000_epochs_abcd-3.npy'), np.array(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060fea2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24047b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47890d75",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cf23e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f65f88",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc14790",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f9918",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368d65f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34826db",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4db762",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947bc9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c780903",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aca928",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2391712a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec3dd1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f1853",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364e1e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf6ea2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82aa9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cvae_decoder.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca85f02",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afef92",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_encoder.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e8445",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa08e75",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736193c2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.600797,
   "end_time": "2023-03-27T06:11:56.868837",
   "environment_variables": {},
   "exception": true,
   "input_path": "09-abcd-train-cvae.ipynb",
   "output_path": "09-abcd-train-cvae-papermill-3.ipynb",
   "parameters": {},
   "start_time": "2023-03-27T06:11:19.268040",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}